<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Basics · DifferentiationInterface.jl</title><meta name="title" content="Basics · DifferentiationInterface.jl"/><meta property="og:title" content="Basics · DifferentiationInterface.jl"/><meta property="twitter:title" content="Basics · DifferentiationInterface.jl"/><meta name="description" content="Documentation for DifferentiationInterface.jl."/><meta property="og:description" content="Documentation for DifferentiationInterface.jl."/><meta property="twitter:description" content="Documentation for DifferentiationInterface.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="DifferentiationInterface.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">DifferentiationInterface.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Tutorials</span><ul><li class="is-active"><a class="tocitem" href>Basics</a><ul class="internal"><li><a class="tocitem" href="#Computing-a-gradient"><span>Computing a gradient</span></a></li><li><a class="tocitem" href="#Overwriting-a-gradient"><span>Overwriting a gradient</span></a></li><li><a class="tocitem" href="#Preparing-for-multiple-gradients"><span>Preparing for multiple gradients</span></a></li><li><a class="tocitem" href="#Switching-backends"><span>Switching backends</span></a></li></ul></li><li><a class="tocitem" href="../tutorial2/">Sparsity</a></li></ul></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../operators/">Operators</a></li><li><a class="tocitem" href="../backends/">Backends</a></li><li><a class="tocitem" href="../api/">API</a></li></ul></li><li><span class="tocitem">Advanced</span><ul><li><a class="tocitem" href="../preparation/">Preparation</a></li><li><a class="tocitem" href="../overloads/">Overloads</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Basics</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Basics</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/gdalle/DifferentiationInterface.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/gdalle/DifferentiationInterface.jl/blob/main/DifferentiationInterface/docs/src/tutorial1.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Basics"><a class="docs-heading-anchor" href="#Basics">Basics</a><a id="Basics-1"></a><a class="docs-heading-anchor-permalink" href="#Basics" title="Permalink"></a></h1><p>We present the main features of DifferentiationInterface.jl.</p><pre><code class="language-julia hljs">using DifferentiationInterface</code></pre><h2 id="Computing-a-gradient"><a class="docs-heading-anchor" href="#Computing-a-gradient">Computing a gradient</a><a id="Computing-a-gradient-1"></a><a class="docs-heading-anchor-permalink" href="#Computing-a-gradient" title="Permalink"></a></h2><p>A common use case of automatic differentiation (AD) is optimizing real-valued functions with first- or second-order methods. Let&#39;s define a simple objective and a random input vector</p><pre><code class="language-julia hljs">f(x) = sum(abs2, x)

x = collect(1.0:5.0)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5-element Vector{Float64}:
 1.0
 2.0
 3.0
 4.0
 5.0</code></pre><p>To compute its gradient, we need to choose a &quot;backend&quot;, i.e. an AD package to call under the hood. Most backend types are defined by <a href="https://github.com/SciML/ADTypes.jl">ADTypes.jl</a> and re-exported by DifferentiationInterface.jl.</p><p><a href="https://github.com/JuliaDiff/ForwardDiff.jl">ForwardDiff.jl</a> is very generic and efficient for low-dimensional inputs, so it&#39;s a good starting point:</p><pre><code class="language-julia hljs">import ForwardDiff

backend = AutoForwardDiff()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">AutoForwardDiff{nothing, Nothing}(nothing)</code></pre><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>To avoid name conflicts, load AD packages with <code>import</code> instead of <code>using</code>. Indeed, most AD packages also export operators like <code>gradient</code> and <code>jacobian</code>, but you only want to use the ones from DifferentiationInterface.jl.</p></div></div><p>Now you can use the following syntax to compute the gradient:</p><pre><code class="language-julia hljs">gradient(f, backend, x)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5-element Vector{Float64}:
  2.0
  4.0
  6.0
  8.0
 10.0</code></pre><p>Was that fast? <a href="https://github.com/JuliaCI/BenchmarkTools.jl">BenchmarkTools.jl</a> helps you answer that question.</p><pre><code class="language-julia hljs">using BenchmarkTools

@benchmark gradient($f, $backend, $x)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">BenchmarkTools.Trial: 10000 samples with 187 evaluations.
 Range <span class="sgr90">(</span><span class="sgr36"><span class="sgr1">min</span></span> … <span class="sgr35">max</span><span class="sgr90">):  </span><span class="sgr36"><span class="sgr1">553.604 ns</span></span> … <span class="sgr35"> 32.256 μs</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>min … max<span class="sgr90">): </span>0.00% … 96.75%
 Time  <span class="sgr90">(</span><span class="sgr34"><span class="sgr1">median</span></span><span class="sgr90">):     </span><span class="sgr34"><span class="sgr1">566.620 ns               </span></span><span class="sgr90">┊</span> GC <span class="sgr90">(</span>median<span class="sgr90">):    </span>0.00%
 Time  <span class="sgr90">(</span><span class="sgr32"><span class="sgr1">mean</span></span> ± <span class="sgr32">σ</span><span class="sgr90">):   </span><span class="sgr32"><span class="sgr1">593.129 ns</span></span> ± <span class="sgr32">533.883 ns</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>mean ± σ<span class="sgr90">):  </span>3.33% ±  4.43%

  ▁▇█<span class="sgr34">▇</span>▅▃▁ <span class="sgr32"> </span>    ▁▁▁                                              ▂
  ███<span class="sgr34">█</span>███▇<span class="sgr32">▇</span>▆▅▆█████▅▅▆▆▅▅▅▆▅▆▅▁▃▄▄▁▄▄▄▄▄▃▃▄▃▃▄▁▁▃▃▁▃▄▄▁▃▁▄▄▃▁▃▄ █
  554 ns<span class="sgr90">        Histogram: <span class="sgr1">log(</span>frequency<span class="sgr1">)</span> by time</span>        855 ns <span class="sgr1">&lt;</span>

 Memory estimate<span class="sgr90">: </span><span class="sgr33">848 bytes</span>, allocs estimate<span class="sgr90">: </span><span class="sgr33">4</span>.</code></pre><p>Not bad, but you can do better.</p><h2 id="Overwriting-a-gradient"><a class="docs-heading-anchor" href="#Overwriting-a-gradient">Overwriting a gradient</a><a id="Overwriting-a-gradient-1"></a><a class="docs-heading-anchor-permalink" href="#Overwriting-a-gradient" title="Permalink"></a></h2><p>Since you know how much space your gradient will occupy (the same as your input <code>x</code>), you can pre-allocate that memory and offer it to AD. Some backends get a speed boost from this trick.</p><pre><code class="language-julia hljs">grad = similar(x)
gradient!(f, grad, backend, x)
grad  # has been mutated</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5-element Vector{Float64}:
  2.0
  4.0
  6.0
  8.0
 10.0</code></pre><p>The bang indicates that one of the arguments of <code>gradient!</code> might be mutated. More precisely, our convention is that <em>every positional argument between the function and the backend is mutated (and the <code>extras</code> too, see below)</em>.</p><pre><code class="language-julia hljs">@benchmark gradient!($f, _grad, $backend, $x) evals=1 setup=(_grad=similar($x))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">BenchmarkTools.Trial: 10000 samples with 1 evaluation.
 Range <span class="sgr90">(</span><span class="sgr36"><span class="sgr1">min</span></span> … <span class="sgr35">max</span><span class="sgr90">):  </span><span class="sgr36"><span class="sgr1">561.000 ns</span></span> … <span class="sgr35"> 3.757 μs</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>min … max<span class="sgr90">): </span>0.00% … 0.00%
 Time  <span class="sgr90">(</span><span class="sgr34"><span class="sgr1">median</span></span><span class="sgr90">):     </span><span class="sgr34"><span class="sgr1">601.000 ns              </span></span><span class="sgr90">┊</span> GC <span class="sgr90">(</span>median<span class="sgr90">):    </span>0.00%
 Time  <span class="sgr90">(</span><span class="sgr32"><span class="sgr1">mean</span></span> ± <span class="sgr32">σ</span><span class="sgr90">):   </span><span class="sgr32"><span class="sgr1">623.114 ns</span></span> ± <span class="sgr32">79.193 ns</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>mean ± σ<span class="sgr90">):  </span>0.00% ± 0.00%

   ▃ ▇█▅<span class="sgr34">█</span>▁▇▅<span class="sgr32"> </span>▄▃ ▂ ▂▃ ▃ ▂▂ ▂▁ ▂ ▂▁ ▁  ▁ ▁▁                      ▂
  ▄█▁███<span class="sgr34">█</span>███<span class="sgr32">▁</span>████▁██▁█▁██▁████▁██▁██▇█▁██▁█▁▇▇▁▇▄▆▆▁█▇▁▆▄▆█▁▇▅ █
  561 ns<span class="sgr90">        Histogram: <span class="sgr1">log(</span>frequency<span class="sgr1">)</span> by time</span>       932 ns <span class="sgr1">&lt;</span>

 Memory estimate<span class="sgr90">: </span><span class="sgr33">752 bytes</span>, allocs estimate<span class="sgr90">: </span><span class="sgr33">3</span>.</code></pre><p>For some reason the in-place version is not much better than your first attempt. However, it makes fewer allocations, thanks to the gradient vector you provided. Don&#39;t worry, you can get even more performance.</p><h2 id="Preparing-for-multiple-gradients"><a class="docs-heading-anchor" href="#Preparing-for-multiple-gradients">Preparing for multiple gradients</a><a id="Preparing-for-multiple-gradients-1"></a><a class="docs-heading-anchor-permalink" href="#Preparing-for-multiple-gradients" title="Permalink"></a></h2><p>Internally, ForwardDiff.jl creates some data structures to keep track of things. These objects can be reused between gradient computations, even on different input values. We abstract away the preparation step behind a backend-agnostic syntax:</p><pre><code class="language-julia hljs">extras = prepare_gradient(f, backend, randn(eltype(x), size(x)))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">DifferentiationInterfaceForwardDiffExt.ForwardDiffGradientExtras{ForwardDiff.GradientConfig{ForwardDiff.Tag{typeof(Main.f), Float64}, Float64, 5, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(Main.f), Float64}, Float64, 5}}}}(ForwardDiff.GradientConfig{ForwardDiff.Tag{typeof(Main.f), Float64}, Float64, 5, Vector{ForwardDiff.Dual{ForwardDiff.Tag{typeof(Main.f), Float64}, Float64, 5}}}((Partials(1.0, 0.0, 0.0, 0.0, 0.0), Partials(0.0, 1.0, 0.0, 0.0, 0.0), Partials(0.0, 0.0, 1.0, 0.0, 0.0), Partials(0.0, 0.0, 0.0, 1.0, 0.0), Partials(0.0, 0.0, 0.0, 0.0, 1.0)), ForwardDiff.Dual{ForwardDiff.Tag{typeof(Main.f), Float64}, Float64, 5}[Dual{ForwardDiff.Tag{typeof(Main.f), Float64}}(0.0,0.0,0.0,0.0,0.0,0.0), Dual{ForwardDiff.Tag{typeof(Main.f), Float64}}(0.0,0.0,0.0,0.0,0.0,0.0), Dual{ForwardDiff.Tag{typeof(Main.f), Float64}}(0.0,0.0,0.0,0.0,0.0,0.0), Dual{ForwardDiff.Tag{typeof(Main.f), Float64}}(0.0,0.0,0.0,0.0,0.0,0.0), Dual{ForwardDiff.Tag{typeof(Main.f), Float64}}(0.0,0.0,0.0,0.0,0.0,0.0)]))</code></pre><p>You don&#39;t need to know what this object is, you just need to pass it to the gradient operator. Note that preparation does not depend on the actual components of the vector <code>x</code>, just on its type and size. You can thus reuse the <code>extras</code> for different values of the input.</p><pre><code class="language-julia hljs">grad = similar(x)
gradient!(f, grad, backend, x, extras)
grad  # has been mutated</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5-element Vector{Float64}:
  2.0
  4.0
  6.0
  8.0
 10.0</code></pre><p>Preparation makes the gradient computation much faster, and (in this case) allocation-free.</p><pre><code class="language-julia hljs">@benchmark gradient!($f, _grad, $backend, $x, _extras) evals=1 setup=(
    _grad=similar($x);
    _extras=prepare_gradient($f, $backend, $x)
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">BenchmarkTools.Trial: 10000 samples with 1 evaluation.
 Range <span class="sgr90">(</span><span class="sgr36"><span class="sgr1">min</span></span> … <span class="sgr35">max</span><span class="sgr90">):  </span><span class="sgr36"><span class="sgr1">60.000 ns</span></span> … <span class="sgr35">682.000 ns</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>min … max<span class="sgr90">): </span>0.00% … 0.00%
 Time  <span class="sgr90">(</span><span class="sgr34"><span class="sgr1">median</span></span><span class="sgr90">):     </span><span class="sgr34"><span class="sgr1">80.000 ns               </span></span><span class="sgr90">┊</span> GC <span class="sgr90">(</span>median<span class="sgr90">):    </span>0.00%
 Time  <span class="sgr90">(</span><span class="sgr32"><span class="sgr1">mean</span></span> ± <span class="sgr32">σ</span><span class="sgr90">):   </span><span class="sgr32"><span class="sgr1">81.413 ns</span></span> ± <span class="sgr32"> 25.512 ns</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>mean ± σ<span class="sgr90">):  </span>0.00% ± 0.00%

      █▅   █<span class="sgr34">▅</span><span class="sgr32"> </span>  ▄▂   ▁                        ▁    ▂▁   ▂      ▂
  ▃▁▁▁██▁▁▁█<span class="sgr34">█</span><span class="sgr32">▁</span>▁▁██▁▁▁█▇▁▁▁▇▆▁▁▁▆▄▁▁▁▄▃▁▁▁▆▅▁▁▁██▁▁▁██▁▁▁██▁▁▁▆ █
  60 ns<span class="sgr90">         Histogram: <span class="sgr1">log(</span>frequency<span class="sgr1">)</span> by time</span>       180 ns <span class="sgr1">&lt;</span>

 Memory estimate<span class="sgr90">: </span><span class="sgr33">0 bytes</span>, allocs estimate<span class="sgr90">: </span><span class="sgr33">0</span>.</code></pre><p>Beware that the <code>extras</code> object is nearly always mutated by differentiation operators, even though it is given as the last positional argument.</p><h2 id="Switching-backends"><a class="docs-heading-anchor" href="#Switching-backends">Switching backends</a><a id="Switching-backends-1"></a><a class="docs-heading-anchor-permalink" href="#Switching-backends" title="Permalink"></a></h2><p>The whole point of DifferentiationInterface.jl is that you can easily experiment with different AD solutions. Typically, for gradients, reverse mode AD might be a better fit, so let&#39;s try the state-of-the-art <a href="https://github.com/EnzymeAD/Enzyme.jl">Enzyme.jl</a>!</p><pre><code class="language-julia hljs">import Enzyme

backend2 = AutoEnzyme()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">AutoEnzyme{Nothing}(nothing)</code></pre><p>Once the backend is created, things run smoothly with exactly the same syntax as before:</p><pre><code class="language-julia hljs">gradient(f, backend2, x)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5-element Vector{Float64}:
  2.0
  4.0
  6.0
  8.0
 10.0</code></pre><p>And you can run the same benchmarks to see what you gained (although such a small input may not be realistic):</p><pre><code class="language-julia hljs">@benchmark gradient!($f, _grad, $backend2, $x, _extras) evals=1 setup=(
    _grad=similar($x);
    _extras=prepare_gradient($f, $backend2, $x)
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">BenchmarkTools.Trial: 10000 samples with 1 evaluation.
 Range <span class="sgr90">(</span><span class="sgr36"><span class="sgr1">min</span></span> … <span class="sgr35">max</span><span class="sgr90">):  </span><span class="sgr36"><span class="sgr1">40.000 ns</span></span> … <span class="sgr35">310.000 ns</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>min … max<span class="sgr90">): </span>0.00% … 0.00%
 Time  <span class="sgr90">(</span><span class="sgr34"><span class="sgr1">median</span></span><span class="sgr90">):     </span><span class="sgr34"><span class="sgr1">50.000 ns               </span></span><span class="sgr90">┊</span> GC <span class="sgr90">(</span>median<span class="sgr90">):    </span>0.00%
 Time  <span class="sgr90">(</span><span class="sgr32"><span class="sgr1">mean</span></span> ± <span class="sgr32">σ</span><span class="sgr90">):   </span><span class="sgr32"><span class="sgr1">47.291 ns</span></span> ± <span class="sgr32">  7.851 ns</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>mean ± σ<span class="sgr90">):  </span>0.00% ± 0.00%

  ▂              <span class="sgr32"> </span>   █<span class="sgr34"> </span>                                         
  █▃▁▁▁▁▁▁▁▁▁▁▁▁▁<span class="sgr32">▁</span>▁▃▁█<span class="sgr34">▁</span>▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂ ▂
  40 ns<span class="sgr90">           Histogram: frequency by time</span>           70 ns <span class="sgr1">&lt;</span>

 Memory estimate<span class="sgr90">: </span><span class="sgr33">0 bytes</span>, allocs estimate<span class="sgr90">: </span><span class="sgr33">0</span>.</code></pre><p>In short, DifferentiationInterface.jl allows for easy testing and comparison of AD backends. If you want to go further, check out the <a href="https://gdalle.github.io/DifferentiationInterface.jl/DifferentiationInterfaceTest">documentation of DifferentiationInterfaceTest.jl</a>. This related package provides benchmarking utilities to compare backends and help you select the one that is best suited for your problem.</p><script type="module">import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
mermaid.initialize({
    startOnLoad: true,
    theme: "neutral"
});
</script></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../tutorial2/">Sparsity »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.1 on <span class="colophon-date" title="Monday 27 May 2024 21:45">Monday 27 May 2024</span>. Using Julia version 1.10.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
